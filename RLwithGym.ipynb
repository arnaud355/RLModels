{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import gym # openAi gym\n",
    "from gym import envs\n",
    "import numpy as np \n",
    "import datetime\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from time import sleep\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's start learning using some build-in games in Gym that do not require additonal installs.\n",
    "Let's start with a very basic game called Taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Taxi game\n",
    "There are four designated locations in the grid world indicated by R(ed), B(lue), G(reen), and Y(ellow). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drive to the passenger's location, pick up the passenger, drive to the passenger's destination (another one of the four specified locations), and then drop off the passenger. Once the passenger is dropped off, the episode ends. The taxi cannot pass thru a wall.\n",
    "\n",
    "Actions: There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger\n",
    "\n",
    "Rewards: There is a reward of -1 for each action and an additional reward of +20 for delievering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "\n",
    "Rendering:\n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters: locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interacting with the Gym environment\n",
    "![title](img/agentSchema.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "observation, reward, done, info = env.step(action)\n",
    "\n",
    "observation (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game line Taxi.\n",
    "reward (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "done (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "info (dict): ignore, diagnostic information useful for debugging. Official evaluations of your agent are not allowed to use this for learning.\n",
    "Let's first do some random steps in the game so you see how the game looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "Reward: -42\n"
     ]
    }
   ],
   "source": [
    "# Let's first do some random steps in the game so you see how the game looks like\n",
    "\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "for _ in range(6):\n",
    "    action = env.action_space.sample() #take step using random action from possible actions (actio_space)\n",
    "    obs, rew, done, info = env.step(action) \n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "#Print the reward of these random action\n",
    "print(\"Reward: %r\" % rew_tot)    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Action\n",
    "Action (a): the input the agent provides to the environment. So what are the action commands the agents can give to the enironment? The env.action_space will tell you\n",
    "\n",
    "What is the meaning of the actions? For the deep learning algorithm it should not matter, it should sort it out independent of the meaning of the action. But for humans it is handy to have the description, so we can understand the actions.\n",
    "\n",
    "In case of the Taxi game [0..5]:\n",
    "\n",
    "0: move south\n",
    "1: move north\n",
    "2: move east\n",
    "3: move west\n",
    "4: pickup passenger\n",
    "5: dropoff passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Possible actions: [0..5]\n"
     ]
    }
   ],
   "source": [
    "# action space has 6 possible actions, the meaning of the actions is nice to know for us humans but the neural network will figure it out\n",
    "print(env.action_space)\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "print(\"Possible actions: [0..%a]\" % (NUM_ACTIONS-1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "State\n",
    "State (s): This represents the board state of the game and in gym returned it is returned as observation. State: a numeric representation of what the agent is observing at a particular moment of time in the environment.\n",
    "In case of Taxi the observation is an integer, 500 different states are possible that translate to a nice graphic visual format with the render function. Note that this is specific for the Taxi game, in case of e.g. an Atari game the observation is the game screen with many coloured pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(500)\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print()\n",
    "env.env.s=42 # some random number, you might recognize it\n",
    "env.render()\n",
    "env.env.s = 222 # and some other\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Markov decision process(MDP)\n",
    "The Taxi game is an example of an Markov decision process . The game can be described in states, possible actions in a state (leading to a next state with a certain probability) with a reward.\n",
    "\n",
    "The word Markov refers to Markovian property which means that the state is independent of any previous states history, not on the sequence of events that preceded it. The current state encapsulates all that is needed to decide the future actions, no memory needed.\n",
    "\n",
    "In terms of Reinforcement Learning the Environment is modelled as a markov model and the agent needs to take actions in this environment to maximize the amount of reward. Since the agent sees only the outside of the environment (the effects of it actions) it is often referred to as the hidden markov model which needs to be learned.\n",
    "\n",
    "Policy\n",
    "Policy (π): The strategy that the agent employs to determine next action 'a' in state 's'. Note that it does not state if it is a good or bad policy, it is a policy. The policy is normally noted with the greek letter π. Optimal policy (π*), policy which maximizes the expected reward. Among all the policies taken, the optimal policy is the one that optimizes to maximize the amount of reward received or expected to receive over a lifetime.\n",
    "\n",
    "So how do we find the optimal policy (π*) that is maximize our reward (and win the game) given the Taxi environment with the Markov model ."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bellman equation\n",
    "We will make use of the basic Bellman equation for deterministic environments to solve a problem that is described as a Markov model, see figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bellman.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "R(s,a) = Reward of action a in state s\n",
    "P(s'|s,a)= Probability of going to state s' given action a in state s. The Taxi game actions are deterministic (no such a thing as if I want to go north there is an 80% chance to go north and 10% chance to go west and 10% chance to go east). so the probability that selected action will lead to expected state is 100%. So ignore it for this game, it is always 1.\n",
    "γ = Discount factor gamma, how much discount is applicable for the future rewards. It must be between 0 and 1. The higher gamma the higher the focus on long term rewards\n",
    "The value iteration algorithm makes use of the equation in the form of:\n",
    "\n",
    "Value V(s): The expected long-term return with discount, as opposed to the short-term reward R. Vπ(s) is defined as the expected long-term return of the current state sunder policy π.\n",
    "The Q learning algorithm makes use of the equation in the form of:\n",
    "\n",
    "Q-matrix or action-value Q(s,a): Q-matrix is similar to Value, except that it takes an extra parameter, the action a. Qπ(s, a) refers to the long-term return of the current state s, taking action a under policy π."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Value iteration algorithm\n",
    "So' let's start first in \"memory lane\", the early days of reinforcement learning. Value iteration is the \"hello world\" of reinforcement learning methods to find an optimal policy to maximize reward for e.g. a Markov decision process problem.\n",
    "\n",
    "The value iteration is centred around the game states. The core of the idea is to calculate the value (expected long-term maximum result) of each state. The algorithm loops over all states (s) and possible actions (a) to explore rewards of a given action and calculates the maximum possible action/reward and stores it in V[s]. The algorithm iterates/repeats until V[s] is not (significantly) improving anymore. The Optimal policy (π*) is then to take every time the action to go state with the highest value. This value iteration algorithm is an example of what is referred to as dynamic programming (DP) in literature. There are other DP techniques to solve this like policy iteration, etc but it can also be solved by a recursive program (a function that calls itself, look at the Bellman equation, it is a recursive definition).\n",
    "Anyhow, lets see how this value iteration works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41  iterations done\n"
     ]
    }
   ],
   "source": [
    "# Value iteration algorithem\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "V = np.zeros([NUM_STATES]) # The Value for each state\n",
    "Pi = np.zeros([NUM_STATES], dtype=int)  # Our policy with we keep updating to get the optimal policy\n",
    "gamma = 0.9 # discount factor\n",
    "significant_improvement = 0.01\n",
    "\n",
    "def best_action_value(s):\n",
    "    # finds the highest value action (max_a) in state s\n",
    "    best_a = None\n",
    "    best_value = float('-inf')\n",
    "\n",
    "    # loop through all possible actions to find the best current action\n",
    "    for a in range (0, NUM_ACTIONS):\n",
    "        env.env.s = s\n",
    "        s_new, rew, done, info = env.step(a) #take the action\n",
    "        v = rew + gamma * V[s_new]\n",
    "        if v > best_value:\n",
    "            best_value = v\n",
    "            best_a = a\n",
    "    return best_a\n",
    "\n",
    "iteration = 0\n",
    "while True:\n",
    "    # biggest_change is referred to by the mathematical symbol delta in equations\n",
    "    biggest_change = 0\n",
    "    for s in range (0, NUM_STATES):\n",
    "        old_v = V[s]\n",
    "        action = best_action_value(s) #choosing an action with the highest future reward\n",
    "        env.env.s = s # goto the state\n",
    "        s_new, rew, done, info = env.step(action) #take the action\n",
    "        V[s] = rew + gamma * V[s_new] #Update Value for the state using Bellman equation\n",
    "        Pi[s] = action\n",
    "        biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "    iteration += 1\n",
    "    if biggest_change < significant_improvement:\n",
    "        print (iteration,' iterations done')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | :\u001b[43m \u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Reward: 8\n"
     ]
    }
   ],
   "source": [
    "# Let's see how the algorithm solves the taxi game\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = Pi[obs]\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "#Print the reward of these actions\n",
    "print(\"Reward: %r\" % rew_tot)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model vs Model-free based methods\n",
    "It nicely solves it. Still it feels a bit like cheating in terms of reinforcement learning. We have to know all environment states/transitions upfront so the algorithm works. In RL literature they refer to it as model based methods. What if not all states are known upfront and we need to find out while we are learning. Hence we enter the realm of model-free based methods.\n",
    "\n",
    "Basic Q-learning algorithm\n",
    "The Q-learning algorithm is centred around the actor (in this case the Taxi) and starts exploring based on trial-and-error to update its knowledge about the model and hence path to the best reward. The core of the idea is the Q-matrix Q(s, a). It contains the maximum discounted future reward when we perform action a in state s. Or in other words Q(s, a) gives estimates the best course of action a in state s. Q-learning learns by trail and error and updates its policy (Q-matrix) based on reward. to state it simple: the best it can do given a state it is in.\n",
    "After every step we update Q(s,a) using the reward, and the max Q value for new state resulting from the action. This update is done using the action value formula (based upon the Bellman equation) and allows state-action pairs to be updated in a recursive fashion (based on future values).\n",
    "The Bellman equation is extended with a learning rate (if you put learning rate = 1 it comes back to the basic Bellman equation) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/bellmanQ.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: Temporal difference learning and Sarsa algorithems explored simular value expressions. Q-learning was the basis for Deep Q-learning (Deep referring to Neural Network technology). so, let's see how the Q-learning algorithm works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS]) #You could also make this dynamic if you don't know all games states upfront\n",
    "gamma = 0.9 # discount factor\n",
    "alpha = 0.9 # learning rate\n",
    "for episode in range(1,1001):\n",
    "    done = False\n",
    "    rew_tot = 0\n",
    "    obs = env.reset()\n",
    "    while done != True:\n",
    "            action = np.argmax(Q[obs]) #choosing the action with the highest Q value \n",
    "            obs2, rew, done, info = env.step(action) #take the action\n",
    "            Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-marix using Bellman equation\n",
    "            #Q[obs,action] = rew + gamma * np.max(Q[obs2]) # same equation but with learning rate = 1 returns the basic Bellman equation\n",
    "            rew_tot = rew_tot + rew\n",
    "            obs = obs2   \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode,rew_tot))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So, what is the magic, how does it solve it?\n",
    "\n",
    "The Q-matrix is initialized with zero's. So initially it starts moving randomly until it hits a state/action with rewards or state/actions with a penalty. For understanding, let's simplify the problem that it needs to go to a certain drop-off position to get a reward. So random moves get no rewards but by luck (brute force enough tries) the state/action is found where a reward is given. So next game the immediate actions preceding this state/action will direct toward it by use of the Q-Matrix. The next iteration the actions before that, etc, etc. In other words, it solves \"the puzzle\" backwards from end-result (drop-off passenger) towards steps to be taken to get there in a iterative fashion.\n",
    "\n",
    "Note that in case of the Taxi game there is a reward of -1 for each action. So if in a state the algorithm explored eg south which let to no value the Q-matrix is updated to -1 so next iteration (because values were initialized on 0) it will try an action that is not yet tried and still on 0. So also by design it encourages systematic exploration of states and actions\n",
    "\n",
    "If you put the learning rate on 1 the game also is solved. Reason is that there is only one reward (dropoff passenger), so the algorithm will find it whatever learning rate. In case a game has more reward places the learning rate determines if it should prioritize longer term or shorter term rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the algorithm solves the taxi game by following the policy to take actions delivering max value\n",
    "\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    rew_tot = rew_tot + rew\n",
    "    env.render()\n",
    "#Print the reward of these actions\n",
    "print(\"Reward: %r\" % rew_tot) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "exploration vs. exploitation\n",
    "The taxi game has one place with the reward: dropoff passenger +20. What if it also had a reward: stop at coffee shop +2 points. If the trial-and-error found that value first and in subsequent iterations started to optimize the route to it, how do we know it would also find the bigger reward of dropoff passenger?\n",
    "What if the the Taxi game actions are not deterministic (eg due to slippery roads I want to go north there is an 80% chance to go north and 10% chance to go west and 10% chance to go east), how would we ensure we still find the optimal policy?\n",
    "\n",
    "Our algorithm to exploit action = np.argmax(Q[obs]) time and time again will not cope with these more complex environments. In Reinforcement literature this is called the crucial tradeoff between \"exploitation\" and \"exploration\".\n",
    "\n",
    "Exploitation: Make the best decision given current information (Go to the restaurent you know you like)\n",
    "Exploration: Gather more information (go to a new restaurent to find out if you like it)\n",
    "Some approaches:\n",
    "\n",
    "Epsilon Greedy\n",
    "We exploit the current situation with probability 1 — epsilon and explore a new option with probability epsilon, the rates of exploration and exploitation are fixed\n",
    "Epsilon-Decreasing\n",
    "We exploit the current situation with probability 1 — epsilon and explore a new option with probability epsilon, with epsilon decreasing over time.\n",
    "Thompson sampling: the rates of exploration and exploitation are dynamically updated with respect to the entire probability distribution of each arm\n",
    "Epsilon-Decreasing with Softmax We exploit the current situation with probability 1 — epsilon and explore a new option with probability epsilon, with epsilon decreasing over time. In the case of exploring a new option, we don’t just pick an option at random, but instead we estimate the outcome of each option, and then pick based on that (this is the softmax part).\n",
    "Etc\n",
    "The code below I tried with epsilon-greedy\n",
    "\n",
    "A nice place to test this is in the game Frozen lakes of OpenAI/Gym.\n",
    "\n",
    "Short description: \"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"\n",
    "\n",
    "Notice that the game is not deterministic anymore: \"won't always move in the direction you intend\". Note it is really slippery, the chance you move into the direction you want is actually not that big\n",
    "\n",
    "S- Start\n",
    "G - Goal\n",
    "F- Frozen (safe)\n",
    "H- Hole (dead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show the game layout\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "rew_tot=0\n",
    "obs= env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS]) #You could also make this dynamic if you don't know all games states upfront\n",
    "gamma = 0.95 # discount factor\n",
    "alpha = 0.01 # learning rate\n",
    "epsilon = 0.1 #\n",
    "for episode in range(1,500001):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while done != True:\n",
    "        if np.random.rand(1) < epsilon:\n",
    "            # exploration with a new option with probability epsilon, the epsilon greedy approach\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # exploitation\n",
    "            action = np.argmax(Q[obs])\n",
    "        obs2, rew, done, info = env.step(action) #take the action\n",
    "        Q[obs,action] += alpha * (rew + gamma * np.max(Q[obs2]) - Q[obs,action]) #Update Q-marix using Bellman equation\n",
    "        obs = obs2   \n",
    "        \n",
    "    if episode % 5000 == 0:\n",
    "        #report every 5000 steps, test 100 games to get avarage point score for statistics and verify if it is solved\n",
    "        rew_average = 0.\n",
    "        for i in range(100):\n",
    "            obs= env.reset()\n",
    "            done=False\n",
    "            while done != True: \n",
    "                action = np.argmax(Q[obs])\n",
    "                obs, rew, done, info = env.step(action) #take step using selected action\n",
    "                rew_average += rew\n",
    "        rew_average=rew_average/100\n",
    "        print('Episode {} avarage reward: {}'.format(episode,rew_average))\n",
    "        \n",
    "        if rew_average > 0.8:\n",
    "            # FrozenLake-v0 defines \"solving\" as getting average reward of 0.78 over 100 consecutive trials.\n",
    "            # Test it on 0.8 so it is not a one-off lucky shot solving it\n",
    "            print(\"Frozen lake solved\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the algorithm solves the frozen-lakes game\n",
    "\n",
    "rew_tot=0.\n",
    "obs= env.reset()\n",
    "done=False\n",
    "while done != True: \n",
    "    action = np.argmax(Q[obs])\n",
    "    obs, rew, done, info = env.step(action) #take step using selected action\n",
    "    rew_tot += rew\n",
    "    env.render()\n",
    "\n",
    "print(\"Reward:\", rew_tot)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Analyzing the moves. It looks like that if you want to move rigth there is a significant chance you move up or down, simular if you want to move up there is a significant chance you move left or right, etc. So the algorithm learned that if you are on the frozen tile left column second row and you want to move down it is risky to give the down command because you could move to the right into the hole. So it gives the left command because if will keep you on the tile or move you up or down, but not to thr right.\n",
    "Or in other words, the algorithm learned to take that actions with the least risk to (accidently slip) drown into a hole. Also interesting to se it learned as first move to go left, this to avoid you move right which is the more dangerous road.\n",
    "\n",
    "Note: there is no 100% score possible. By consitently moving away from a hole you can safely traverse all fields except 1 (second row, thrid column) on which you could glide into due to slippery ice.\n",
    "\n",
    "Also good to notice the algorithm uses tenthousands of iterations to find the optimal policy, while this is a 4 by 4 playing field..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DQN\n",
    "Nice, isn't it. In case of 500 observation states and 6 actions (Taxi game) or 16 observations and 4 moves (Frozen-lake game) the Q-matrix is a manageable matrix. Imagine you got a full Atari game screen of pixels as an observation and it becomes quickly visible the Q-matrix solution will not cope. Also the Q-learning agent does not have the ability to estimate value for unseen states, it has no clue which action to take and goes back to random action as best.\n",
    "\n",
    "To deal with these problems, Deep Q-Network (DQN) removes the two-dimensional Q-matrix by introducing a Neural Network. So it leverages a Neural Network to estimate the Q-value function. The input for the network is the current game state, while the output is the corresponding Q-value for each of the actions.\n",
    "alt text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/dq.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In 2014 Google DeepMind published a paper titled \"Playing Atari with Deep Reinforcement Learning\" that can play Atari 2600 games at expert human levels. This was the first breakthruogh in applying deep neural networks for reinforcement learning.\n",
    "Reinforcement learning developments¶\n",
    "After the first publication of DQN many deeplearning Reinforcement Learning algorithms have been invented/tried, Some main ones in chronological order: DQN, Double DQN, Duelling DQN, Deep Deterministic Policy Gradient, Continuous DQN (CDQN or NAF) , A2C/A3C, Proximal Policy Optimization Algorithms, etc, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlwithgym",
   "language": "python",
   "name": "rlwithgym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
